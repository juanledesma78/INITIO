import subprocess as sb
from pathlib import Path 
import argparse
import os
import pandas as pd
import glob
import re
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from Bio import SeqIO

program_specs = argparse.ArgumentParser(
    prog='post-processing_of_genomancer_results.py',
    usage='%(prog)s -i  -w  [-initio]',
    description='The script %(prog)s decompresses the files *_quasibams.zip,\
                 *_post-run.zip and *_typonomer.zip, creates an excel file to\
                 summarise the results generated by the pipeline Genomancer and\
                 extracts each fasta sequence from the file *_genomes.fas into\
                 a new directory called FASTAs.\
                 USE ONLY ON HIV INITIO PROJECT DATA: The parameter initio\
                 provides two additional FASTA sequences at 2 and 20PC of nucleotide\
                 frequency and depth of 100 reads and additional details to the excel\
                 file')
program_specs.add_argument('--input','-i', required=True, 
                help='path to the NGS run containing the data generated by Genomancer')
program_specs.add_argument('--workflow','-w', required= True, 
                help ='Workflow used to generate the data using Genomancer\
                      (i.e, HIV, HCV, EV...')
program_specs.add_argument('-initio', action='store_true', 
                help ='OPTIONAL argument ONLY used for the HIV INITIO project.\
                       No VALUE is required')

args = program_specs.parse_args()
args.workflow = args.workflow.upper() 
path_to_run = Path.cwd().joinpath(args.input)
os.chdir(path_to_run)
runid = path_to_run.name

##########################################################################
# UNZIP FOLDER
##########################################################################

if Path.cwd().joinpath('quasibams').exists() == False:
    quasibam = sb.run(['unzip',f'{args.workflow}_quasibams.zip','-d', 'quasibams'])
if Path.cwd().joinpath('post-run').exists() == False:
    postrun = sb.run(['unzip',f'{args.workflow}_post-run.zip','-d', 'post-run'])
if Path.cwd().joinpath('typonomer').exists() == False:
    typonomer = sb.run(['unzip',f'{args.workflow}_typonomer.zip','-d', 'typonomer'])

##########################################################################
# GENERATION OF DEPTH FILE USING QUASIBAM TAB FILES
##########################################################################

#os.chdir(os.path.join(path_to_run, 'quasibam'))

#file_names = glob.glob ("quasibams/*.tabular")
frequency_range = [20, 2] 
dataframe_list =[]
for quasibam_file in Path('quasibams').glob('*.tabular'):
    seq_id = quasibam_file.stem
    if args.initio:
        seq_id = re.sub(r'^\d+_','', seq_id) # 'H215141107.1'
    quasi_df = pd.read_csv(quasibam_file , sep = "\t")
    quasi_df.rename(columns={'Depth': seq_id}, inplace = True)
    dataframe_list.append(quasi_df[seq_id])
    if args.initio:
        
        for frequency in frequency_range:
            outfile = Path('quasibams').joinpath(f'{seq_id}.{frequency}PC.fas')
            fasta_header = f'{seq_id}.{frequency}PC'
            running_quasibam_ = sb.run(["qb_post_process.pl",
                                        "-i",quasibam_file,
                                        "-o",outfile,
                                        "-s", fasta_header,
                                        "-d", "100",
                                        "-c", str(frequency),
                                        "-n", "N"])
        #    tabular_output = f'quasibams/{seq_id}.tabular'
        #    os.rename(quasibam_file, tabular_output)
        quasibam_file.rename(Path('quasibams').joinpath(f'{seq_id}.tabular'))
    #print('alternative to running quasibam')
    
Depth_dataframe = pd.concat(dataframe_list, axis = 1 )
Depth_dataframe.index += 1
Depth_dataframe['Position'] = Depth_dataframe.index #using the index as a column
Depth_dataframe.insert(0, 'Position', Depth_dataframe.pop('Position'))


## just for INITIO PROJECT
if args.initio:   
    NGS_metrics = []
    metrics_cols = ['Sample ID', 'Length','Median Depth', 'Max', 
                    'Min', 'Depth 10', 'Depth 30', 'Depth 50', 'Depth 100']
    for col in Depth_dataframe.columns:
        if col != 'Position': 
            column = Depth_dataframe[col]
            coverage10reads = (column[Depth_dataframe[col]>10].count()/Depth_dataframe[col].count())*100
            coverage30reads = (column[Depth_dataframe[col]>30].count()/Depth_dataframe[col].count())*100
            coverage50reads = (column[Depth_dataframe[col]>50].count()/Depth_dataframe[col].count())*100
            coverage100reads = (column[Depth_dataframe[col]>100].count()/Depth_dataframe[col].count())*100
            id = re.search(r'^RS\d+', col)
            id = id.group()
            print(id)
            NGS_metrics.append([
                                id, 
                                Depth_dataframe[col].count(), 
                                Depth_dataframe[col].median(), 
                                Depth_dataframe[col].max(), 
                                Depth_dataframe[col].min(),
                                round(coverage10reads,2), round(coverage30reads,2), 
                                round(coverage50reads,2), round(coverage100reads,2)
                                    ])

    new_dataframe = pd.DataFrame(NGS_metrics, columns=metrics_cols)

##########################################################################
# GENERATION OF SUMMARY REPORT DATAFRAME
##########################################################################

genome_dict = {
        'HCV':['Core', 'E1', 'E2', 'NS3', 'NS4b', 'NS5a', 'NS5b'],
        'HIV':['gag', 'pol', 'vif', 'vpu', 'env', 'nef'],
        'EV':['2C', '3C', '3D', 'VP1', 'VP2', 'VP3']
                }

summary_dataframe = pd.read_csv(f'{args.workflow}_read_counts.tsv', sep = "\t")
summary_dataframe = summary_dataframe.rename(columns={'Unnamed: 0' : 'FASTQ ID'})#'Sample ID'})
summary_dataframe = summary_dataframe.rename(columns={'Raw' : 'Raw reads'})
summary_dataframe = summary_dataframe.rename(columns={'Trimmed' : 'Trimmed reads'})
summary_dataframe = summary_dataframe.rename(columns={'Post-prinseq' : 'Post-prinseq reads'})
summary_dataframe = summary_dataframe.rename(columns={'Dehumanised' : 'Dehumanised reads'})
summary_dataframe = summary_dataframe.rename(columns={'Mapped' : 'Mapped reads'})
summary_dataframe['% Mapped reads'] = round((summary_dataframe['Mapped reads']*100)/summary_dataframe['Raw reads'], 2)
summary_dataframe['Fasta sequences by the pipeline'] = ''

run_sample_list = []
for fastq in summary_dataframe['FASTQ ID']:
    seq_id = re.sub(r'^\d+_','', fastq)
    run_sample_list.append(seq_id)
summary_dataframe['Sample ID'] = run_sample_list #'H220141691'
summary_dataframe.insert(0, 'Sample ID', summary_dataframe.pop('Sample ID'))
fastq_list = list(summary_dataframe['FASTQ ID']) # '1574517_H220141691'

if args.workflow in genome_dict:
    for gene in genome_dict[args.workflow]:
        summary_dataframe[gene] = ""


##########################################################################
# IDENTIFYING THE FASTA SEQUENCES GENERATED BY GENOMANCER
##########################################################################

fasta_sequence_results = []
with open(f'{args.workflow}_genomes.fas','r') as genfas:
    fasta_file = genfas.read()
    #for sample in run_sample_list:
    for fastq in fastq_list:
        #sample = re.sub(r'^\d+_','', sample)
        pattern = f'{fastq}\.\d+'
        sequence_ids = re.findall(pattern, fasta_file)
        info =""
        for genome in sequence_ids:
                if len(info)>0:
                    info += " ; " + genome
                else:
                    info += genome
        fasta_sequence_results.append(info)

summary_dataframe['Fasta sequences by the pipeline'] = fasta_sequence_results

FASTAs = Path.cwd().joinpath(f'FASTAs')
if FASTAs.is_dir() is False:
    os.mkdir(FASTAs)
for record in SeqIO.parse(Path.cwd().joinpath(f'{args.workflow}_genomes.fas'), 'fasta'):
    SeqIO.write(record, FASTAs/f'{record.id}.fas', 'fasta')

##########################################################################
# FINDING OUT WHETHER THE SEQUENCES ARE INCLUDED IN THE TREES
##########################################################################

for phylo_file in Path('post-run').glob('*.nex'):
    with open(phylo_file, 'r') as newick:
        gene = (newick.name).replace('post-run/','').replace('.nex','')
        tree = newick.read()
        for n in range(len(summary_dataframe)):
            row_info = ''
            genomes = summary_dataframe.loc[n]['Fasta sequences by the pipeline'].split(' ; ')
            for i in range(len(genomes)):
                if genomes[i] in tree:
                    if len(row_info)>0:
                        row_info += " ; " + genomes[i]
                    else:
                        row_info += genomes[i]

            summary_dataframe.loc[n, [gene]] = row_info
    #phylo_file.rename(Path('post-run').joinpath(f'{runid}_{phylo_file.name}'))

if args.initio:
    summary_dataframe = pd.merge(summary_dataframe, new_dataframe, how= 'left', on='Sample ID')
#summary_dataframe['Fasta sequences by the pipeline'].replace('', 'N/A', inplace = True)
summary_dataframe.replace('', 'N/A', inplace = True)
summary_dataframe.replace('NaN', 'N/A', inplace = True)

##########################################################################
# GENERATION OF EXCEL FILE CONTAINING SUMMARY AND DEPTH
##########################################################################

workbook = Workbook()
ws = workbook.active
ws1 = workbook.create_sheet(f'Summary NGS Genomancer', 0)
ws2 = workbook.create_sheet(f'Depth', 1)

for rowdf in dataframe_to_rows(summary_dataframe, index=False, header=True):
    ws1.append(rowdf)
for cell in ws1[1]:
    cell.style = 'Accent1'

for rowdf in dataframe_to_rows(Depth_dataframe, index=False, header=True):
    ws2.append(rowdf)
for cell in ws2[1]:
    cell.style = 'Accent1'

excel_file = Path.cwd().joinpath(f'Summary_Genomancer_Results_{runid}_{args.workflow}.xlsx') 
workbook.save(excel_file)
